{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ernestoruizds/DS_C3_SC1_ERNESTO_RUIZ_SANCHEZ/blob/main/DS_C3_SC1_ERNESTO_RUIZ_SANCHEZ.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 503
        },
        "id": "AamQNHdNfutA",
        "outputId": "caf651a1-ceed-4941-de44-7b46c18a544c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Exactitud del modelo Red Neuronal con 2 capas con 5 y 10 neuronas: 0.8177777777777778\n",
            "\n",
            "Matriz de confusi√≥n Red Neuronal de 2 capas con 5 y 10 neuronas\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 2 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAfsAAAGwCAYAAACuFMx9AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9hAAAPYQGoP6dpAAA9IUlEQVR4nO3deXgUVdr38V9nXzsQNN1EkgCiQBRBYQbafYlE5FV4YHR0ohMV0cGACiMKzwCyiFFcUDCCg8gyA+MOI4goooJKQInig4BREA0SOlEhhESTTrrr/QNpbQFN00madH0/11XXlao6p+ruMcOd+5zTVRbDMAwBAICQFRbsAAAAQNMi2QMAEOJI9gAAhDiSPQAAIY5kDwBAiCPZAwAQ4kj2AACEuIhgBxAIj8ej0tJSJSYmymKxBDscAICfDMPQgQMHlJqaqrCwpqs/a2pq5HK5Ar5OVFSUYmJiGiGi5tWik31paanS0tKCHQYAIEC7du1Su3btmuTaNTU16pCRIGe5O+Br2e127dy5s8Ul/Bad7BMTEyVJX3/UXtYEZiQQmv7n1G7BDgFoMvWq03ta4f33vCm4XC45y936uqi9rInHnisqD3iU0fMruVwukn1zOjR0b00IC+g/IHA8i7BEBjsEoOn89MD25piKTUi0KCHx2O/jUcudLm7RyR4AgIZyGx65A3gbjNvwNF4wzYxkDwAwBY8MeXTs2T6QvsHG2DcAACGOyh4AYAoeeRTIQHxgvYOLZA8AMAW3YchtHPtQfCB9g41hfAAAQhyVPQDAFMy8QI9kDwAwBY8MuU2a7BnGBwAgxFHZAwBMgWF8AABCHKvxAQBAyKKyBwCYguenLZD+LRXJHgBgCu4AV+MH0jfYSPYAAFNwGwrwrXeNF0tzY84eAIAQR2UPADAF5uwBAAhxHlnkliWg/i0Vw/gAAIQ4KnsAgCl4jINbIP1bKpI9AMAU3AEO4wfSN9gYxgcAIMRR2QMATMHMlT3JHgBgCh7DIo8RwGr8APoGG8P4AACEOCp7AIApMIwPAECIcytM7gAGtN2NGEtzI9kDAEzBCHDO3mDOHgAAHK+o7AEApsCcPQAAIc5thMltBDBn34Ifl8swPgAAIY7KHgBgCh5Z5AmgxvWo5Zb2JHsAgCmYec6eYXwAAEIclT0AwBQCX6DXcofxqewBAKZwcM4+sM0fbrdb48ePV4cOHRQbG6uTTz5ZU6ZMkfGLPxoMw9CECRPUtm1bxcbGKisrS1988YXPdfbu3aucnBxZrVa1atVKQ4YMUVVVlV+xkOwBAGgCDz74oGbNmqUnnnhC27Zt04MPPqhp06Zp5syZ3jbTpk3TjBkzNHv2bG3YsEHx8fHKzs5WTU2Nt01OTo62bNmiVatWafny5Vq7dq1uueUWv2JhGB8AYAqeAJ+N7+9q/HXr1mnAgAHq37+/JKl9+/b6z3/+ow8++EDSwar+scce07hx4zRgwABJ0sKFC2Wz2bR06VJdc8012rZtm1auXKkPP/xQvXr1kiTNnDlTl19+uR5++GGlpqY2KBYqewCAKRyasw9kk6TKykqfrba29oj3O/vss7V69Wp9/vnnkqRPPvlE7733nvr16ydJ2rlzp5xOp7Kysrx9kpKS1Lt3bxUWFkqSCgsL1apVK2+il6SsrCyFhYVpw4YNDf7sVPYAAFPwKKxRvmeflpbmc/zee+/VxIkTD2s/ZswYVVZWqkuXLgoPD5fb7dbUqVOVk5MjSXI6nZIkm83m089ms3nPOZ1OpaSk+JyPiIhQcnKyt01DkOwBAPDDrl27ZLVavfvR0dFHbPf8889r0aJFWrx4sU477TRt2rRJd955p1JTU5Wbm9tc4Uoi2QMATMJtWOQO4DW1h/parVafZH80o0eP1pgxY3TNNddIkrp166avv/5a+fn5ys3Nld1ulySVlZWpbdu23n5lZWXq0aOHJMlut6u8vNznuvX19dq7d6+3f0MwZw8AMAX3Twv0Atn88cMPPygszLdPeHi4PB6PJKlDhw6y2+1avXq193xlZaU2bNggh8MhSXI4HKqoqFBRUZG3zVtvvSWPx6PevXs3OBYqewAAmsAVV1yhqVOnKj09Xaeddpo+/vhjPfroo7rpppskSRaLRXfeeafuu+8+nXLKKerQoYPGjx+v1NRUDRw4UJLUtWtXXXbZZRo6dKhmz56turo6DR8+XNdcc02DV+JLJHsAgEl4jDB5AniCnsfPJ+jNnDlT48eP12233aby8nKlpqbq1ltv1YQJE7xt7r77blVXV+uWW25RRUWFzj33XK1cuVIxMTHeNosWLdLw4cN1ySWXKCwsTIMHD9aMGTP8isViGC33+X+VlZVKSkrSvs87yprIjARCU3Zqj2CHADSZeqNO7+i/2r9/f4PmwY/FoVwx56OeiksMP+br/HDAraFnFTVprE2FDAkAQIhjGB8AYAoeKaDV+J7GC6XZkewBAKYQ+EN1Wu5geMuNHAAANAiVPQDAFAJ/n33LrY9J9gAAUziWd9L/un9LRbIHAJiCmSv7lhs5AABoECp7AIApHMvz7X/dv6Ui2QMATMFjWOQJ5Hv2AfQNtpb7ZwoAAGgQKnsAgCl4AhzGb8kP1SHZAwBMIfC33rXcZN9yIwcAAA1CZQ8AMAW3LHIH8GCcQPoGG8keAGAKDOMDAICQRWUPADAFtwIbinc3XijNjmQPADAFMw/jk+wBAKbAi3AAAEDIorIHAJiCEeD77A2+egcAwPGNYXwAABCyqOwBAKZg5lfckuwBAKbgDvCtd4H0DbaWGzkAAGgQKnsAgCkwjA8AQIjzKEyeAAa0A+kbbC03cgAA0CBU9gAAU3AbFrkDGIoPpG+wkewBAKbAnD0AACHOCPCtdwZP0AMAAMcrKnsAgCm4ZZE7gJfZBNI32Ej2AABT8BiBzbt7jEYMppkxjA8AQIijsjc5t1v69yN2rX6ptfZ9G6k2tjpdevVe/eXOMll++gP4x+owzZ3aVoWvJ6lyX4TsaS4NGPKt/t9fvz/seoYhjbuuoza+bdW9c3fq7H77m/kTAYc7vXeVrrrtW53S7Qe1sddr4k3tVbgyyXv+79NL1PfP+3z6bHw7Uf/I6ejdnzh/p04+7Ue1alOvA/vD9fG7iZo7ta32lkU22+dAYDwBLtALpG+wkexN7vmCFC1fcILuerxEGZ1r9MUnsXpkZLriE90aePN3kqSnJqZq0/uJuntmiWxpLn20JlEzx7ZTG1udHNmVPtdbMudE7x8JwPEiJs6jL7fE6PX/JOveZ746YpsP30rUIyPTvPt1Lt9f5E/eT9CzM1K0tyxSJ7St09AJpRo/5yuNvPKUpgwdjcgjizwBzLsH0jfYjos/UwoKCtS+fXvFxMSod+/e+uCDD4Idkmls3RgvR/Z+9c6qlD3NpfP+336ddcEBFW+K82lz6VV71f3sKtnTXLr8uu/VMfNHnzaStOPTWL301Ika9WhJc38M4DdtfNuqBdPaat0vqvlfq3NZtO/bSO9Wtd+3Floy50R99lG8yndHaevGeD33RIq6nPWDwiNa8EQuTCPoyf65557TqFGjdO+99+qjjz5S9+7dlZ2drfLy8mCHZgqZvaq16b1EfbMjWpK0Y0uMtnwQrz9cfMCnzfo3kvTdnkgZhrTp/QTt/jJaPS/4uU3NDxY9kJehvKnfKDmlvtk/BxCoMxxVeu7/tujpdz/TiPxvlNj66L/Hia3qdfGgfdq6MU7u+pZb7ZnNoSfoBbL5o3379rJYLIdteXl5kqSamhrl5eWpTZs2SkhI0ODBg1VWVuZzjZKSEvXv319xcXFKSUnR6NGjVV/v/7+xQR/Gf/TRRzV06FDdeOONkqTZs2fr1Vdf1TPPPKMxY8YEObrQ9+fh5frhQLhuPr+LwsIlj1u6YcweXTzo5/nL2+7brcfvTlNOz9MUHmEoLMzQHQ/tUrc+1d42T008SZm9qnX2ZZVHug1wXNv4TqLefy1JzpIotW3v0o1j9mjqv7/UnVecIo/n53/gh/yjVFfe+L1i4jzaujFOE3I7BDFq+Ku55+w//PBDud1u7/6nn36qSy+9VFdddZUkaeTIkXr11Vf1wgsvKCkpScOHD9egQYP0/vvvS5Lcbrf69+8vu92udevWac+ePfrrX/+qyMhI3X///X7FEtRk73K5VFRUpLFjx3qPhYWFKSsrS4WFhYe1r62tVW1trXe/spLEEqi1r7TSWy+31piCr5XRuUY7tsRq9r0n/bRQ72DC/+8zJ+izojhNmv+lUtq5tHl9ggr+9+Cc/VnnV6nwdas2vZ+oJ98oDvKnAY7Nmv+29v781Wex2rk1RgvWf6Yzzq7SpvcSvedemJWilf9pI1s7l3JGOTX68RJN+GsHqQXP5cJ/v8490dHRio6OPqzdiSee6LP/wAMP6OSTT9YFF1yg/fv3a+7cuVq8eLEuvvhiSdK8efPUtWtXrV+/Xn369NEbb7yhrVu36s0335TNZlOPHj00ZcoU3XPPPZo4caKioqIaHHNQh/G/++47ud1u2Ww2n+M2m01Op/Ow9vn5+UpKSvJuaWlph7WBf+ZMSdWfh5frwoEV6tC1Rll/2qdBQ7/VszMP/jep/dGi+Q+01S0TS9Wnb6U6ZtZowE3f6YIrK/Ti7BRJ0qb3E7XnqygN6tJN/dK6q19ad0nSlKHtNXpwp6B9NuBYOUuiVfF9uFLbu3yOV+6N0O4vo/XR2kTlD8tQ76wD6trzhyBFCX95ZPE+H/+Ytp/+qEtLS/PJRfn5+b97b5fLpX//+9+66aabZLFYVFRUpLq6OmVlZXnbdOnSRenp6d5it7CwUN26dfPJkdnZ2aqsrNSWLVv8+uxBH8b3x9ixYzVq1CjvfmVlJQk/QLU1YbKE+S4wCgs3ZPx0qL7eovq6MIUdqY3n4M9/Hl6mfn/x/RrerRd30a0Td6tPX0Zf0PKc0NYla2u39pYf/Z9Iy0+lUmQUC/RaCiPA1fjGT3137dolq9XqPX6kqv7Xli5dqoqKCt1www2SJKfTqaioKLVq1cqn3S+LXafTecRi+NA5fwQ12Z9wwgkKDw8/bEFCWVmZ7Hb7Ye2PNlSCY9fn0ko9O8OmlJPqDg7jfxqrl59KUd9rDibv+ESPznBUac6UVEXF7JatnUv/V5igN19M1i337pYkJafUH3FRXspJdbKnuw47DjS3mDi3Ujv8/LtoT3Op42k/6kBFuA7sC9d1fy/Te68maV95pNq2r9XN4/aodGeUit45OITf+cxqde7xoz79IF5VFeFq275WuXc7VbozStuK4o52WxxnGuutd1ar1SfZN8TcuXPVr18/paamHvP9AxHUZB8VFaWePXtq9erVGjhwoCTJ4/Fo9erVGj58eDBDM43b7vtGC6a11RNj26ni+wi1sdXp8uu/U87In/8AGzvrKz1zf1s9ODxdByoilHKSSzfcs+eID9UBjkendv9RD720w7v/t0mlkqQ3nmutmWPbqUPXH3XpVfsUb3Xr+7IIfbQmUQum2VXnOli+1/4YpnP67df1f3cqJs6jveWR2vh2oqY+bvO2AY7m66+/1ptvvqmXX37Ze8xut8vlcqmiosKnuv9lsWu32w/7Kvqh4vhIBfFvCfow/qhRo5Sbm6tevXrpj3/8ox577DFVV1d7V+ejacUleDRs8m4Nm7z7qG2SU+p112O7/Lru66WbAowMaDz/V5ig7NTuRz3/j7+c/Jv9v/osVvdc/dttcPwL1hP05s2bp5SUFPXv3997rGfPnoqMjNTq1as1ePBgSVJxcbFKSkrkcDgkSQ6HQ1OnTlV5eblSUg6ukVq1apWsVqsyMzP9iiHoyf7Pf/6zvv32W02YMEFOp1M9evTQypUrD5unAAAgEI01jO9XH49H8+bNU25uriIifk65SUlJGjJkiEaNGqXk5GRZrVaNGDFCDodDffr0kST17dtXmZmZuv766zVt2jQ5nU6NGzdOeXl5fk9pBz3ZS9Lw4cMZtgcAhJw333xTJSUluummmw47N336dIWFhWnw4MGqra1Vdna2nnzySe/58PBwLV++XMOGDZPD4VB8fLxyc3M1efJkv+M4LpI9AABNLRjPxu/bt68M48jf2IiJiVFBQYEKCgqO2j8jI0MrVqzw+76/RrIHAJhCMIbxjxcsIwUAIMRR2QMATMHMlT3JHgBgCmZO9gzjAwAQ4qjsAQCmYObKnmQPADAFQ8f29blf9m+pSPYAAFMwc2XPnD0AACGOyh4AYApmruxJ9gAAUzBzsmcYHwCAEEdlDwAwBTNX9iR7AIApGIZFRgAJO5C+wcYwPgAAIY7KHgBgCsF4n/3xgmQPADAFM8/ZM4wPAECIo7IHAJiCmRfokewBAKZg5mF8kj0AwBTMXNkzZw8AQIijsgcAmIIR4DB+S67sSfYAAFMwJBlGYP1bKobxAQAIcVT2AABT8MgiC0/QAwAgdLEaHwAAhCwqewCAKXgMiyw8VAcAgNBlGAGuxm/By/EZxgcAIMRR2QMATMHMC/RI9gAAUyDZAwAQ4sy8QI85ewAAQhyVPQDAFMy8Gp9kDwAwhYPJPpA5+0YMppkxjA8AQIijsgcAmIKZV+NT2QMATMFohM1fu3fv1nXXXac2bdooNjZW3bp108aNG3+OyTA0YcIEtW3bVrGxscrKytIXX3zhc429e/cqJydHVqtVrVq10pAhQ1RVVeVXHCR7AACawL59+3TOOecoMjJSr732mrZu3apHHnlErVu39raZNm2aZsyYodmzZ2vDhg2Kj49Xdna2ampqvG1ycnK0ZcsWrVq1SsuXL9fatWt1yy23+BULw/gAAFNo7mH8Bx98UGlpaZo3b573WIcOHX5xPUOPPfaYxo0bpwEDBkiSFi5cKJvNpqVLl+qaa67Rtm3btHLlSn344Yfq1auXJGnmzJm6/PLL9fDDDys1NbVBsVDZAwDMoZHG8SsrK3222traI97ulVdeUa9evXTVVVcpJSVFZ555pubMmeM9v3PnTjmdTmVlZXmPJSUlqXfv3iosLJQkFRYWqlWrVt5EL0lZWVkKCwvThg0bGvzRSfYAAHP4qbI/1k0/VfZpaWlKSkrybvn5+Ue83ZdffqlZs2bplFNO0euvv65hw4bp9ttv14IFCyRJTqdTkmSz2Xz62Ww27zmn06mUlBSf8xEREUpOTva2aQiG8QEA8MOuXbtktVq9+9HR0Uds5/F41KtXL91///2SpDPPPFOffvqpZs+erdzc3GaJ9RAqewCAKRx6gl4gmyRZrVaf7WjJvm3btsrMzPQ51rVrV5WUlEiS7Ha7JKmsrMynTVlZmfec3W5XeXm5z/n6+nrt3bvX26YhSPYAAFMIZAj/WBb3nXPOOSouLvY59vnnnysjI0PSwcV6drtdq1ev9p6vrKzUhg0b5HA4JEkOh0MVFRUqKirytnnrrbfk8XjUu3fvBsfCMD4AAE1g5MiROvvss3X//ffr6quv1gcffKB//vOf+uc//ylJslgsuvPOO3XffffplFNOUYcOHTR+/HilpqZq4MCBkg6OBFx22WUaOnSoZs+erbq6Og0fPlzXXHNNg1fiSyR7AIBZ/GKR3TH398Mf/vAHLVmyRGPHjtXkyZPVoUMHPfbYY8rJyfG2ufvuu1VdXa1bbrlFFRUVOvfcc7Vy5UrFxMR42yxatEjDhw/XJZdcorCwMA0ePFgzZszwKxaLYbTcR/tXVlYqKSlJ+z7vKGsiMxIITdmpPYIdAtBk6o06vaP/av/+/T6L3hrToVyR8fR4hcXF/H6Ho/D8UKOvb57SpLE2FTIkAAAhjmF8AIA5HOsD7n/Zv4Ui2QMATMHMb71rULJ/5ZVXGnzBK6+88piDAQAAja9Byf7QVwB+j8VikdvtDiQeAACaTgseig9Eg5K9x+Np6jgAAGhSZh7GD2g1/i/ftwsAwHGtkd561xL5nezdbremTJmik046SQkJCfryyy8lSePHj9fcuXMbPUAAABAYv5P91KlTNX/+fE2bNk1RUVHe46effrqefvrpRg0OAIDGY2mErWXyO9kvXLhQ//znP5WTk6Pw8HDv8e7du+uzzz5r1OAAAGg0DOM33O7du9WpU6fDjns8HtXV1TVKUAAAoPH4newzMzP17rvvHnb8xRdf1JlnntkoQQEA0OhMXNn7/QS9CRMmKDc3V7t375bH49HLL7+s4uJiLVy4UMuXL2+KGAEACFwzv/XueOJ3ZT9gwAAtW7ZMb775puLj4zVhwgRt27ZNy5Yt06WXXtoUMQIAgAAc07PxzzvvPK1ataqxYwEAoMkYxsEtkP4t1TG/CGfjxo3atm2bpIPz+D179my0oAAAaHS89a7hvvnmG1177bV6//331apVK0lSRUWFzj77bD377LNq165dY8cIAAAC4Pec/c0336y6ujpt27ZNe/fu1d69e7Vt2zZ5PB7dfPPNTREjAACBO7RAL5CthfK7sl+zZo3WrVunzp07e4917txZM2fO1HnnndeowQEA0FgsxsEtkP4tld/JPi0t7YgPz3G73UpNTW2UoAAAaHQmnrP3exj/oYce0ogRI7Rx40bvsY0bN+qOO+7Qww8/3KjBAQCAwDWosm/durUslp/nKqqrq9W7d29FRBzsXl9fr4iICN10000aOHBgkwQKAEBATPxQnQYl+8cee6yJwwAAoImZeBi/Qck+Nze3qeMAAABN5JgfqiNJNTU1crlcPsesVmtAAQEA0CRMXNn7vUCvurpaw4cPV0pKiuLj49W6dWufDQCA45KJ33rnd7K/++679dZbb2nWrFmKjo7W008/rUmTJik1NVULFy5sihgBAEAA/B7GX7ZsmRYuXKgLL7xQN954o8477zx16tRJGRkZWrRokXJycpoiTgAAAmPi1fh+V/Z79+5Vx44dJR2cn9+7d68k6dxzz9XatWsbNzoAABrJoSfoBbK1VH4n+44dO2rnzp2SpC5duuj555+XdLDiP/RiHAAAcPzwO9nfeOON+uSTTyRJY8aMUUFBgWJiYjRy5EiNHj260QMEAKBRmHiBnt9z9iNHjvT+nJWVpc8++0xFRUXq1KmTzjjjjEYNDgAABC6g79lLUkZGhjIyMhojFgAAmoxFAb71rtEiaX4NSvYzZsxo8AVvv/32Yw4GAAA0vgYl++nTpzfoYhaLJSjJfnCPPyrCEtXs9wWaw47FpwY7BKDJeH6okYb8t3luZuKv3jUo2R9afQ8AQIvF43IBAECoCniBHgAALYKJK3uSPQDAFAJ9Cp6pnqAHAABaFpI9AMAcmvkJehMnTpTFYvHZunTp4j1fU1OjvLw8tWnTRgkJCRo8eLDKysp8rlFSUqL+/fsrLi5OKSkpGj16tOrr6/3+6MeU7N99911dd911cjgc2r17tyTpX//6l957771juRwAAE0vCI/LPe2007Rnzx7v9ss8OXLkSC1btkwvvPCC1qxZo9LSUg0aNMh73u12q3///nK5XFq3bp0WLFig+fPna8KECX7H4Xeyf+mll5Sdna3Y2Fh9/PHHqq2tlSTt379f999/v98BAADQklRWVvpsh/LgkURERMhut3u3E044QdLBnDl37lw9+uijuvjii9WzZ0/NmzdP69at0/r16yVJb7zxhrZu3ap///vf6tGjh/r166cpU6aooKBALpfLr5j9Tvb33XefZs+erTlz5igyMtJ7/JxzztFHH33k7+UAAGgWjfWK27S0NCUlJXm3/Pz8o97ziy++UGpqqjp27KicnByVlJRIkoqKilRXV6esrCxv2y5duig9PV2FhYWSpMLCQnXr1k02m83bJjs7W5WVldqyZYtfn93v1fjFxcU6//zzDzuelJSkiooKfy8HAEDzaKQn6O3atUtWq9V7ODo6+ojNe/furfnz56tz587as2ePJk2apPPOO0+ffvqpnE6noqKiDns1vM1mk9PplCQ5nU6fRH/o/KFz/vA72dvtdm3fvl3t27f3Of7ee++pY8eO/l4OAIDm0Ujfs7darT7J/mj69evn/fmMM85Q7969lZGRoeeff16xsbEBBOI/v4fxhw4dqjvuuEMbNmyQxWJRaWmpFi1apLvuukvDhg1rihgBAGjxWrVqpVNPPVXbt2+X3W6Xy+U6bES8rKxMdrtd0sHi+ter8w/tH2rTUH4n+zFjxugvf/mLLrnkElVVVen888/XzTffrFtvvVUjRozw93IAADSLxpqzP1ZVVVXasWOH2rZtq549eyoyMlKrV6/2ni8uLlZJSYkcDockyeFwaPPmzSovL/e2WbVqlaxWqzIzM/26t9/D+BaLRf/4xz80evRobd++XVVVVcrMzFRCQoK/lwIAoPk08+Ny77rrLl1xxRXKyMhQaWmp7r33XoWHh+vaa69VUlKShgwZolGjRik5OVlWq1UjRoyQw+FQnz59JEl9+/ZVZmamrr/+ek2bNk1Op1Pjxo1TXl7eUdcJHM0xPy43KirK778sAAAwi2+++UbXXnutvv/+e5144ok699xztX79ep144omSDr4+PiwsTIMHD1Ztba2ys7P15JNPevuHh4dr+fLlGjZsmBwOh+Lj45Wbm6vJkyf7HYvfyf6iiy6SxXL01YxvvfWW30EAANDkAh2K97Pvs88++5vnY2JiVFBQoIKCgqO2ycjI0IoVK/y78RH4nex79Ojhs19XV6dNmzbp008/VW5ubsABAQDQJHjrXcNNnz79iMcnTpyoqqqqgAMCAACNq9FehHPdddfpmWeeaazLAQDQuILwbPzjRaO9z76wsFAxMTGNdTkAABqVmd9n73ey/+UbeSTJMAzt2bNHGzdu1Pjx4xstMAAA0Dj8TvZJSUk++2FhYercubMmT56svn37NlpgAACgcfiV7N1ut2688UZ169ZNrVu3bqqYAABofCZeje/XAr3w8HD17duXt9sBAFqcYD8uN5j8Xo1/+umn68svv2yKWAAAQBPwO9nfd999uuuuu7R8+XLt2bNHlZWVPhsAAMctE37tTvJjzn7y5Mn6+9//rssvv1ySdOWVV/o8NtcwDFksFrnd7saPEgCAQJl4zr7ByX7SpEn629/+prfffrsp4wEAAI2swcneMA7+SXPBBRc0WTAAADQVHqrTQL/1tjsAAI5rDOM3zKmnnvq7CX/v3r0BBQQAABqXX8l+0qRJhz1BDwCAloBh/Aa65pprlJKS0lSxAADQdEw8jN/g79kzXw8AQMvk92p8AABaJBNX9g1O9h6PpynjAACgSTFnDwBAqDNxZe/3s/EBAEDLQmUPADAHE1f2JHsAgCmYec6eYXwAAEIclT0AwBwYxgcAILQxjA8AAEIWlT0AwBwYxgcAIMSZONkzjA8AQIijsgcAmILlpy2Q/i0VyR4AYA4mHsYn2QMATIGv3gEAgJBFZQ8AMAeG8QEAMIEWnLADwTA+AAAhjsoeAGAKLNADACDUGY2wHaMHHnhAFotFd955p/dYTU2N8vLy1KZNGyUkJGjw4MEqKyvz6VdSUqL+/fsrLi5OKSkpGj16tOrr6/2+P8keAIAm9OGHH+qpp57SGWec4XN85MiRWrZsmV544QWtWbNGpaWlGjRokPe82+1W//795XK5tG7dOi1YsEDz58/XhAkT/I6BZA8AMIVDw/iBbP6qqqpSTk6O5syZo9atW3uP79+/X3PnztWjjz6qiy++WD179tS8efO0bt06rV+/XpL0xhtvaOvWrfr3v/+tHj16qF+/fpoyZYoKCgrkcrn8ioNkDwAwh0Yaxq+srPTZamtrj3rLvLw89e/fX1lZWT7Hi4qKVFdX53O8S5cuSk9PV2FhoSSpsLBQ3bp1k81m87bJzs5WZWWltmzZ4tdHJ9kDAOCHtLQ0JSUlebf8/Pwjtnv22Wf10UcfHfG80+lUVFSUWrVq5XPcZrPJ6XR62/wy0R86f+icP1iNDwAwhcZajb9r1y5ZrVbv8ejo6MPa7tq1S3fccYdWrVqlmJiYY79pI6GyBwCYQyMN41utVp/tSMm+qKhI5eXlOuussxQREaGIiAitWbNGM2bMUEREhGw2m1wulyoqKnz6lZWVyW63S5Lsdvthq/MP7R9q01AkewCAOTTjV+8uueQSbd68WZs2bfJuvXr1Uk5OjvfnyMhIrV692tunuLhYJSUlcjgckiSHw6HNmzervLzc22bVqlWyWq3KzMz066MzjA8AQCNLTEzU6aef7nMsPj5ebdq08R4fMmSIRo0apeTkZFmtVo0YMUIOh0N9+vSRJPXt21eZmZm6/vrrNW3aNDmdTo0bN055eXlHHE34LSR7AIApHG9P0Js+fbrCwsI0ePBg1dbWKjs7W08++aT3fHh4uJYvX65hw4bJ4XAoPj5eubm5mjx5st/3ItkDAMwhyG+9e+edd3z2Y2JiVFBQoIKCgqP2ycjI0IoVKwK7sZizBwAg5FHZAwBMwWIYshjHXp4H0jfYSPYAAHMI8jB+MDGMDwBAiKOyBwCYwvG2Gr85kewBAObAMD4AAAhVVPYAAFNgGB8AgFBn4mF8kj0AwBTMXNkzZw8AQIijsgcAmAPD+AAAhL6WPBQfCIbxAQAIcVT2AABzMIyDWyD9WyiSPQDAFFiNDwAAQhaVPQDAHFiNDwBAaLN4Dm6B9G+pGMYHACDEUdlDp/+hUn8aWqpOp1Wpja1Ok//WWYVvJnvPv7a98Ij9nn4gXS89fZIkaf47H8nWrtbn/DMPpeuFp05qusCBBmj94h4lv1zmc8zVNlq7HukqSQqvqFObxaWK3XxAYTUe1bWN1r6BNlX/sZW3feSeGrVZXKqY4mpZ3IZq02K19yq7ak5LbM6PgkAxjA8zi4l168ttcXrjhRM1ftbnh53/S5+ePvu9LqjQnfk79P7rbXyOL5yeppXPpXj3f6gOb5qAAT+52sWo9H9P9u4bYRbvzymzShRW7Zbz7x3kToxQwrp9sj3+lb6Zeqpc7eMkSfaHdqrOHq3ScZ1kRIYpaeW3avvwTpVM7yp3q8hm/zw4NqzGD5K1a9fqiiuuUGpqqiwWi5YuXRrMcExr49rWWjg9XetWtTni+X3fRflsfbL26v/WW+XcFePT7sfqcJ92tT+S7HF8MMIld6tI7+ax/lznxHxerf3ZJ6i2U7zqbdGq+B+7PPHhit75oyQprLJeUc5aVVyZIld6rOraRuv7a9oqrNajqF01wfpIOBaHvmcfyNZCBTXZV1dXq3v37iooKAhmGPBDqzYu/fHCCr3+Qsph5666dbee+/BDPfHKJxp8826Fhbfc/2MgtEQ6Xcq47VOl37FVKU98rYjvXN5zNafGK2F9hcKq6iWPoYR1+2SpM/Rj1wRJkicxXK620Up8d68sNW7Jbci6+nvVWyNU2yE2WB8J8EtQh/H79eunfv36Nbh9bW2tamt/nheurKxsirDwG7IGfasfq8MOG8L/70K7tm+J14GKCGWedUA33FWi5JQ6zbm/fXACBX5S2yle5bfGqi41WuH76pT8slOpk7/Qrge7yIgNV9ntGbLN+FodbvlURrjkiQqTc2R71dujD17AYlHp/54s+6M71WHIZskiua0R2jOmozwJzIS2JGYexm9Rv6n5+fmaNGlSsMMwtb5/Ktfbr5yoOpfvoNCSZ1K9P39VHK/6ujCNmPKl5j+cflhboDn90MP68056rPZ0ilP67VuVsL5CBy5qo+QXnAr7wa3S/z1Z7sQIxW/cL9uMr1Q64RS50mMlw9CJ87+R2xqh0gmd5IkKk/Xt79X24Z36Zsqpcrdmzr7FMPECvRb1r/DYsWO1f/9+77Zr165gh2Qqp/WqVNrJNVr5/OFD+L/22ScJiog0lHJS7e+2BZqTJz5CdW2jFVlWq4iyWiW98Z3Kb03Tj6cnypURq32D7artECfrqu8kSbFbqhT3UaXKRrRXTecEuTrE6bub0uSJtCjx3b1B/jRAw7Soyj46OlrR0dHBDsO0sq8q1+eb47Xzs/jfbXty12q53dL+76l6cHyx1LgVWeZS1bmRCqv96SkpFotvozCL9wEqlkNtfl0ahVmkFvyQFTNiGB+mFhPnVmrGz6uKbWk16ti1WgcqIvTtnoN/XMUl1Ou8ft9rTn7GYf27nHlAXbpX6ZP1Vv1YHa6uZx7QLf/4Sm//90RVVfIrhuBqs2i3qs9KUv0JkQrfV6/kF/dIYdKBs1vLExculy1KJ87dpe//kuodxo/99ICcd3WUJNWcEi9PfLhSZpVo3yC7jCiLrG99r8hyl3440/o7d8dxhbfewcxO6ValaYu2evdv/cfXkqRVL52oR+/pJEm6oP/3kkV6Z9kJh/Wvc1l0wf/7Tjm371JklEdl38RoybxULXmmbfN8AOA3hH9fJ9vMrxRe5ZbbGqEfT43XN5NP9X79znn3yUp+tlT2h3cqrNajOluUyv+W7k3kHmuE9ow5WcnP7VHq1O2yuA25ToqR8+8d5MpgNT5ahqAm+6qqKm3fvt27v3PnTm3atEnJyclKT08PYmTmsnlDkvp1cvxmm9ees+m152xHPLdjS4JG/qlbU4QGBKz89va/eb6ubbTKRnb4zTa1HeO0Z+zJv9kGxz+G8YNk48aNuuiii7z7o0aNkiTl5uZq/vz5QYoKABCSTLwaP6jJ/sILL5TRgudAAABoCZizBwCYAsP4AACEOo9xcAukfwtFsgcAmIOJ5+xb1BP0AACA/6jsAQCmYFGAc/aNFknzI9kDAMzBxE/QYxgfAIAQR7IHAJjCoa/eBbL5Y9asWTrjjDNktVpltVrlcDj02muvec/X1NQoLy9Pbdq0UUJCggYPHqyysjKfa5SUlKh///6Ki4tTSkqKRo8erfr6er8/O8keAGAORiNsfmjXrp0eeOABFRUVaePGjbr44os1YMAAbdmyRZI0cuRILVu2TC+88ILWrFmj0tJSDRo0yNvf7Xarf//+crlcWrdunRYsWKD58+drwoQJfn905uwBAPBDZWWlz/7RXr9+xRVX+OxPnTpVs2bN0vr169WuXTvNnTtXixcv1sUXXyxJmjdvnrp27ar169erT58+euONN7R161a9+eabstls6tGjh6ZMmaJ77rlHEydOVFRUVINjprIHAJiCxTAC3iQpLS1NSUlJ3i0/P/937+12u/Xss8+qurpaDodDRUVFqqurU1ZWlrdNly5dlJ6ersLCQklSYWGhunXrJpvt55eQZWdnq7Ky0js60FBU9gAAc/D8tAXSX9KuXbtktVq9h49U1R+yefNmORwO1dTUKCEhQUuWLFFmZqY2bdqkqKgotWrVyqe9zWaT0+mUJDmdTp9Ef+j8oXP+INkDAOCHQwvuGqJz587atGmT9u/frxdffFG5ublas2ZNE0d4OJI9AMAUfjkUf6z9/RUVFaVOnTpJknr27KkPP/xQjz/+uP785z/L5XKpoqLCp7ovKyuT3W6XJNntdn3wwQc+1zu0Wv9Qm4Zizh4AYA7NvBr/SDwej2pra9WzZ09FRkZq9erV3nPFxcUqKSmRw+GQJDkcDm3evFnl5eXeNqtWrZLValVmZqZf96WyBwCYQzM/QW/s2LHq16+f0tPTdeDAAS1evFjvvPOOXn/9dSUlJWnIkCEaNWqUkpOTZbVaNWLECDkcDvXp00eS1LdvX2VmZur666/XtGnT5HQ6NW7cOOXl5f3mOoEjIdkDANAEysvL9de//lV79uxRUlKSzjjjDL3++uu69NJLJUnTp09XWFiYBg8erNraWmVnZ+vJJ5/09g8PD9fy5cs1bNgwORwOxcfHKzc3V5MnT/Y7FpI9AMAUjuUpeL/u74+5c+f+5vmYmBgVFBSooKDgqG0yMjK0YsUK/258BCR7AIA58CIcAAAQqqjsAQCmYPEc3ALp31KR7AEA5sAwPgAACFVU9gAAcwj0wTgtt7An2QMAzCEYj8s9XjCMDwBAiKOyBwCYg4kX6JHsAQDmYCiw99m33FxPsgcAmANz9gAAIGRR2QMAzMFQgHP2jRZJsyPZAwDMwcQL9BjGBwAgxFHZAwDMwSPJEmD/FopkDwAwBVbjAwCAkEVlDwAwBxMv0CPZAwDMwcTJnmF8AABCHJU9AMAcTFzZk+wBAObAV+8AAAhtfPUOAACELCp7AIA5MGcPAECI8xiSJYCE7Wm5yZ5hfAAAQhyVPQDAHBjGBwAg1AWY7NVykz3D+AAAhDgqewCAOTCMDwBAiPMYCmgontX4AADgeEVlDwAwB8NzcAukfwtFsgcAmANz9gAAhDjm7AEAQKiisgcAmIOJh/Gp7AEA5mDo54R/TJt/t8vPz9cf/vAHJSYmKiUlRQMHDlRxcbFPm5qaGuXl5alNmzZKSEjQ4MGDVVZW5tOmpKRE/fv3V1xcnFJSUjR69GjV19f7FQvJHgCAJrBmzRrl5eVp/fr1WrVqlerq6tS3b19VV1d724wcOVLLli3TCy+8oDVr1qi0tFSDBg3ynne73erfv79cLpfWrVunBQsWaP78+ZowYYJfsTCMDwAwh2Yexl+5cqXP/vz585WSkqKioiKdf/752r9/v+bOnavFixfr4osvliTNmzdPXbt21fr169WnTx+98cYb2rp1q958803ZbDb16NFDU6ZM0T333KOJEycqKiqqQbFQ2QMAzMHjCXyTVFlZ6bPV1tY26Pb79++XJCUnJ0uSioqKVFdXp6ysLG+bLl26KD09XYWFhZKkwsJCdevWTTabzdsmOztblZWV2rJlS4M/OskeAAA/pKWlKSkpybvl5+f/bh+Px6M777xT55xzjk4//XRJktPpVFRUlFq1auXT1mazyel0etv8MtEfOn/oXEMxjA8AMIdGGsbftWuXrFar93B0dPTvds3Ly9Onn36q995779jvHwCSPQDAHBop2VutVp9k/3uGDx+u5cuXa+3atWrXrp33uN1ul8vlUkVFhU91X1ZWJrvd7m3zwQcf+Fzv0Gr9Q20agmF8AACagGEYGj58uJYsWaK33npLHTp08Dnfs2dPRUZGavXq1d5jxcXFKikpkcPhkCQ5HA5t3rxZ5eXl3jarVq2S1WpVZmZmg2OhsgcAmEMzPy43Ly9Pixcv1n//+18lJiZ659iTkpIUGxurpKQkDRkyRKNGjVJycrKsVqtGjBghh8OhPn36SJL69u2rzMxMXX/99Zo2bZqcTqfGjRunvLy8Bk0fHEKyBwCYgmF4ZATw5jp/+86aNUuSdOGFF/ocnzdvnm644QZJ0vTp0xUWFqbBgwertrZW2dnZevLJJ71tw8PDtXz5cg0bNkwOh0Px8fHKzc3V5MmT/YqFZA8AMAfDCOxlNn7O9xsNaB8TE6OCggIVFBQctU1GRoZWrFjh171/jTl7AABCHJU9AMAcjADn7Fvwi3BI9gAAc/B4JMuxz9krgPn+YGMYHwCAEEdlDwAwB4bxAQAIbYbHIyOAYfxAvrYXbAzjAwAQ4qjsAQDmwDA+AAAhzmNIFnMme4bxAQAIcVT2AABzMAxJgXzPvuVW9iR7AIApGB5DRgDD+A151v3ximQPADAHw6PAKnu+egcAAI5TVPYAAFNgGB8AgFBn4mH8Fp3sD/2VVW/UBTkSoOl4fqgJdghAk/H8WCupearmetUF9EyderXcXNOik/2BAwckSWt/fCnIkQBNaEiwAwCa3oEDB5SUlNQk146KipLdbtd7zhUBX8tutysqKqoRompeFqMFT0J4PB6VlpYqMTFRFosl2OGYQmVlpdLS0rRr1y5ZrdZghwM0Kn6/m59hGDpw4IBSU1MVFtZ0a8ZramrkcrkCvk5UVJRiYmIaIaLm1aIr+7CwMLVr1y7YYZiS1WrlH0OELH6/m1dTVfS/FBMT0yKTdGPhq3cAAIQ4kj0AACGOZA+/REdH695771V0dHSwQwEaHb/fCFUteoEeAAD4fVT2AACEOJI9AAAhjmQPAECII9kDABDiSPZosIKCArVv314xMTHq3bu3Pvjgg2CHBDSKtWvX6oorrlBqaqosFouWLl0a7JCARkWyR4M899xzGjVqlO6991599NFH6t69u7Kzs1VeXh7s0ICAVVdXq3v37iooKAh2KECT4Kt3aJDevXvrD3/4g5544glJB99LkJaWphEjRmjMmDFBjg5oPBaLRUuWLNHAgQODHQrQaKjs8btcLpeKioqUlZXlPRYWFqasrCwVFhYGMTIAQEOQ7PG7vvvuO7ndbtlsNp/jNptNTqczSFEBABqKZA8AQIgj2eN3nXDCCQoPD1dZWZnP8bKyMtnt9iBFBQBoKJI9fldUVJR69uyp1atXe495PB6tXr1aDocjiJEBABoiItgBoGUYNWqUcnNz1atXL/3xj3/UY489purqat14443BDg0IWFVVlbZv3+7d37lzpzZt2qTk5GSlp6cHMTKgcfDVOzTYE088oYceekhOp1M9evTQjBkz1Lt372CHBQTsnXfe0UUXXXTY8dzcXM2fP7/5AwIaGckeAIAQx5w9AAAhjmQPAECII9kDABDiSPYAAIQ4kj0AACGOZA8AQIgj2QMAEOJI9gAAhDiSPRCgG264QQMHDvTuX3jhhbrzzjubPY533nlHFotFFRUVR21jsVi0dOnSBl9z4sSJ6tGjR0BxffXVV7JYLNq0aVNA1wFw7Ej2CEk33HCDLBaLLBaLoqKi1KlTJ02ePFn19fVNfu+XX35ZU6ZMaVDbhiRoAAgUL8JByLrssss0b9481dbWasWKFcrLy1NkZKTGjh17WFuXy6WoqKhGuW9ycnKjXAcAGguVPUJWdHS07Ha7MjIyNGzYMGVlZemVV16R9PPQ+9SpU5WamqrOnTtLknbt2qWrr75arVq1UnJysgYMGKCvvvrKe023261Ro0apVatWatOmje6++279+vUSvx7Gr62t1T333KO0tDRFR0erU6dOmjt3rr766ivvy1dat24ti8WiG264QdLBVwjn5+erQ4cOio2NVffu3fXiiy/63GfFihU69dRTFRsbq4suusgnzoa65557dOqppyouLk4dO3bU+PHjVVdXd1i7p556SmlpaYqLi9PVV1+t/fv3+5x/+umn1bVrV8XExKhLly568skn/Y4FQNMh2cM0YmNj5XK5vPurV69WcXGxVq1apeXLl6uurk7Z2dlKTEzUu+++q/fff18JCQm67LLLvP0eeeQRzZ8/X88884zee+897d27V0uWLPnN+/71r3/Vf/7zH82YMUPbtm3TU089pYSEBKWlpemll16SJBUXF2vPnj16/PHHJUn5+flauHChZs+erS1btmjkyJG67rrrtGbNGkkH/ygZNGiQrrjiCm3atEk333yzxowZ4/f/JomJiZo/f762bt2qxx9/XHPmzNH06dN92mzfvl3PP/+8li1bppUrV+rjjz/Wbbfd5j2/aNEiTZgwQVOnTtW2bdt0//33a/z48VqwYIHf8QBoIgYQgnJzc40BAwYYhmEYHo/HWLVqlREdHW3cdddd3vM2m82ora319vnXv/5ldO7c2fB4PN5jtbW1RmxsrPH6668bhmEYbdu2NaZNm+Y9X1dXZ7Rr1857L8MwjAsuuMC44447DMMwjOLiYkOSsWrVqiPG+fbbbxuSjH379nmP1dTUGHFxcca6det82g4ZMsS49tprDcMwjLFjxxqZmZk+5++5557DrvVrkowlS5Yc9fxDDz1k9OzZ07t/7733GuHh4cY333zjPfbaa68ZYWFhxp49ewzDMIyTTz7ZWLx4sc91pkyZYjgcDsMwDGPnzp2GJOPjjz8+6n0BNC3m7BGyli9froSEBNXV1cnj8egvf/mLJk6c6D3frVs3n3n6Tz75RNu3b1diYqLPdWpqarRjxw7t379fe/bsUe/evb3nIiIi1KtXr8OG8g/ZtGmTwsPDdcEFFzQ47u3bt+uHH37QpZde6nPc5XLpzDPPlCRt27bNJw5JcjgcDb7HIc8995xmzJihHTt2qKqqSvX19bJarT5t0tPTddJJJ/ncx+PxqLi4WImJidqxY4eGDBmioUOHetvU19crKSnJ73gANA2SPULWRRddpFmzZikqKkqpqamKiPD9dY+Pj/fZr6qqUs+ePbVo0aLDrnXiiSceUwyxsbF+96mqqpIkvfrqqz5JVjq4DqGxFBYWKicnR5MmTVJ2draSkpL07LPP6pFHHvE71jlz5hz2x0d4eHijxQogMCR7hKz4+Hh16tSpwe3POussPffcc0pJSTmsuj2kbdu22rBhg84//3xJByvYoqIinXXWWUds361bN3k8Hq1Zs0ZZWVmHnT80suB2u73HMjMzFR0drZKSkqOOCHTt2tW72PCQ9evX//6H/IV169YpIyND//jHP7zHvv7668PalZSUqLS0VKmpqd77hIWFqXPnzrLZbEpNTdWXX36pnJwcv+4PoPmwQA/4SU5Ojk444QQNGDBA7777rnbu3Kl33nlHt99+u7755htJ0h133KEHHnhAS5cu1WeffabbbrvtN78j3759e+Xm5uqmm27S0qVLvdd8/vnnJUkZGRmyWCxavny5vv32W1VVVSkxMVF33XWXRo4cqQULFmjHjh366KOPNHPmTO+it7/97W/64osvNHr0aBUXF2vx4sWaP3++X5/3lFNOUUlJiZ599lnt2LFDM2bMOOJiw5iYGOXm5uqTTz7Ru+++q9tvv11XX3217Ha7JGnSpEnKz8/XjBkz9Pnnn2vz5s2aN2+eHn30Ub/iAdB0SPbAT+Li4rR27Vqlp6dr0KBB6tq1q4YMGaKamhpvpf/3v/9d119/vXJzc+VwOJSYmKj/+Z//+c3rzpo1S3/605902223qUuXLho6dKiqq6slSSeddJImTZqkMWPGyGazafjw4ZKkKVOmaPz48crPz1fXrl112WWX6dVXX1WHDh0kHZxHf+mll7R06VJ1795ds2fP1v333+/X573yyis1cuRIDR8+XD169NC6des0fvz4w9p16tRJgwYN0uWXX66+ffvqjDPO8Plq3c0336ynn35a8+bNU7du3XTBBRdo/vz53lgBBJ/FONrKIgAAEBKo7AEACHEkewAAQhzJHgCAEEeyBwAgxJHsAQAIcSR7AABCHMkeAIAQR7IHACDEkewBAAhxJHsAAEIcyR4AgBD3/wGn1aEGidxsBAAAAABJRU5ErkJggg==\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "#1.\tDeber√°s utilizar el archivo llamado bank_marketing.csv. con los datos de problema.\n",
        "#2.\tUtilizar el archivo bank-names.txt para obtener informaci√≥n de cada una de las variables.\n",
        "#3.\tCrear un proyecto tipo Jupyter Notebook en Google-Colab llamado Solucion_Reto_SC_63_ERNESTO_RUIZ.ipynb.\n",
        "#4.\tIncluye las librer√≠as que consideres adecuadas y carga los datos del archivo en una variable llamada ‚Äúdata‚Äù.\n",
        "\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "from google.colab            import drive\n",
        "from sklearn.preprocessing   import LabelEncoder, OneHotEncoder #, StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model    import LogisticRegression\n",
        "from sklearn.neural_network  import MLPClassifier\n",
        "\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "\n",
        "data = pd.read_csv(\"/content/drive/MyDrive/M17/Reto/bank_marketing_RETO_DS_AS.csv\")\n",
        "\n",
        "# 5.\tObtener la informaci√≥n de dicha base de datos que incluya el n√∫mero de registros,\n",
        "#     el total de variables, el tipo de cada variable, la cantidad de datos perdidos de cada variable en caso de que existan.\n",
        "\n",
        "data.head()\n",
        "data.info()\n",
        "\n",
        "# 6.\tTransforma las variables categ√≥ricas de manera que puedan ser tratadas num√©ricamente. Justifica si utilizas LabelEncoder o OneHotEcoder.\n",
        "\n",
        "# Analizamos los distintos valores de cada variable categ√≥rica\n",
        "data['job'].value_counts()       # Contiene 12 clases distintas\n",
        "data['marital'].value_counts()   # Contiene 3 clases distintas\n",
        "data['education'].value_counts() # Contiene 4 clases distintas\n",
        "data['default'].value_counts()   # Contiene 2 clases distintas (binaria)\n",
        "data['housing'].value_counts()   # Contiene 2 clases distintas (binaria)\n",
        "data['loan'].value_counts()      # Contiene 2 clases distintas (binaria)\n",
        "data['contact'].value_counts()   # Contiene 3 clases distintas\n",
        "data['month'].value_counts()     # Contiene 12 clases distintas\n",
        "data['poutcome'].value_counts()  # Contiene 4 clases distintas\n",
        "\n",
        "# Para variables categ√≥ricas con 4 categor√≠as o menos, utilizamos el m√©todo de One-Hot\n",
        "# porque las Redes Neuronales interpretan m√°s f√°cilmente las salidas One-Hot que directamente los enteros.\n",
        "# Sin embargo, pueden volverse impr√°cticos con variables que tienen muchos valores distintos\n",
        "# porque se crea una columna por cada valor en la variable\n",
        "\n",
        "# Inicializando OneHotEncoder\n",
        "encoder = OneHotEncoder(sparse_output=False)\n",
        "\n",
        "# Aplicamos one-hot encoding a las columnas con pocos valores distintintos y se crea un DataFrame con esas columnas ya transformadas\n",
        "one_hot_encoded  = encoder.fit_transform(data[['marital','education','default','housing','loan','poutcome','contact']])\n",
        "one_hot_df = pd.DataFrame(one_hot_encoded, columns=encoder.get_feature_names_out(['marital','education','default','housing','loan','poutcome','contact']))\n",
        "\n",
        "# Se concatena el dataframe de one-hot encoded con el dataframe original. Este nuevo dataframe se llamar√° df_encoded\n",
        "data = pd.concat([data, one_hot_df], axis=1)\n",
        "\n",
        "# Luego quitamos las columnas que fueron transformadas porque no aportan valor al an√°lisis\n",
        "data = data.drop(['marital','education','default','housing','loan','poutcome','contact'],axis=1)\n",
        "\n",
        "# Para variables con 4 o m√°s de categor√≠as, utilizamos el m√©todo de LabelEncoder; as√≠ como para la variable de salida \"y\"\n",
        "le = LabelEncoder()\n",
        "cols=['job', 'month', 'y']\n",
        "\n",
        "data[cols] = data[cols].apply(LabelEncoder().fit_transform)\n",
        "\n",
        "# 7.\tTransforma las variables num√©ricas en los casos que se tenga alg√∫n tipo de sesgo.\n",
        "\n",
        "# Revisamos las caracter√≠sticas de las variables num√©ricas (age, balance, day, duration, campaign, pdays, previous)\n",
        "data['age'].describe() # Min=18, Max=99\n",
        "data['age'].value_counts()\n",
        "\n",
        "data['balance'].describe() # Min=-3058; Max=81204\n",
        "data['balance'].value_counts()\n",
        "\n",
        "data['day'].describe() # Min=1 , Max=31\n",
        "data['day'].value_counts()\n",
        "\n",
        "data['duration'].describe() # Min=3, Max=3253\n",
        "data['duration'].value_counts()\n",
        "\n",
        "data['campaign'].describe() # Min=1 , Max=58\n",
        "data['campaign'].value_counts()\n",
        "\n",
        "data['pdays'].describe() # Min= -1, Max= 850\n",
        "data['pdays'].value_counts()\n",
        "\n",
        "data['previous'].describe() # Min=0, Max=58\n",
        "data['previous'].value_counts()\n",
        "\n",
        "# Como podemos ver, las variables \"balance\" tiene valores muy grandes y peque√±os incluyendo el cero\n",
        "# De igual modo, vemos que la variable \"pdays\" tiene valores -1 y valores positivos relativamente alejados del -1\n",
        "# Las variables \"age\", \"day\", \"duration\", \"campaign\" y \"previous\" tienen valores positivos pero necesitamos ver su distribuci√≥n\n",
        "\n",
        "# Ahora graficamos las variables para ver si tienen alg√∫n sesgo en su distribuci√≥n\n",
        "fig, axs = plt.subplots(2,4, figsize=(10,7))\n",
        "\n",
        "age      = np.array(data['age'])\n",
        "balance  = np.array(data['balance'])\n",
        "day      = np.array(data['day'])\n",
        "duration = np.array(data['duration'])\n",
        "campaign = np.array(data['campaign'])\n",
        "pdays    = np.array(data['pdays'])\n",
        "previous = np.array(data['previous'])\n",
        "\n",
        "axs[0,0].hist(age, bins=40)\n",
        "axs[0,0].set_xlabel('Age')\n",
        "\n",
        "axs[0,1].hist(balance, bins=20)\n",
        "axs[0,1].set_xlabel('Balance')\n",
        "\n",
        "axs[0,2].hist(day, bins=40)\n",
        "axs[0,2].set_xlabel('Day')\n",
        "\n",
        "axs[0,3].hist(duration, bins=40)\n",
        "axs[0,3].set_xlabel('Duration')\n",
        "\n",
        "axs[1,0].hist(campaign, bins=20)\n",
        "axs[1,0].set_xlabel('campaign')\n",
        "\n",
        "axs[1,1].hist(pdays, bins=40)\n",
        "axs[1,1].set_xlabel('pDays')\n",
        "\n",
        "axs[1,2].hist(previous, bins=40)\n",
        "axs[1,2].set_xlabel('Previous')\n",
        "\n",
        "plt.show()\n",
        "\n",
        "# Se oberva que las varibles \"age\", \"day\", \"duration\" y \"campaign\" tienen un marcado sesgo positivo (a la derecha)\n",
        "# as√≠ que se les aplicar√° una transformaci√≥n logar√≠tmica\n",
        "# Tambi√©n se puede ver que la variable \"day\" no muestra una distribuci√≥n de campana\n",
        "# La variable \"balance\" tiene valores muy grandes y peque√±os, incluyendo el cero\n",
        "# Hay que considerar que la variable \"pdays\" tiene valores negativos\n",
        "# y la variale \"previous\" tiene muchos datos con valor 0\n",
        "\n",
        "# Dado que la variable balance tiene valores muy grandes y peque√±os, incluyendo el cero\n",
        "# Se opta por Escalar de manera Standard\n",
        "data['balance'] = (data['balance'] - data['balance'].mean()) / data['balance'].std()\n",
        "Tbalance        = np.array(data['balance'])\n",
        "\n",
        "# Dado que pDays tiene valores negativos, se opta pusar el m√©todo de min - max para transformar los datos\n",
        "data['pdays'] = (data['pdays'] - min(data['pdays'])) / (max(data['pdays']) - min(data['pdays']))\n",
        "Tdays         = np.array(data['pdays'])\n",
        "\n",
        "# Dado que \"previous\" tiene muchos datos con valor 0,se decide escalar con estandarizaci√≥n\n",
        "data['previous'] = (data['previous'] - data['previous'].mean()) / data['previous'].std()\n",
        "Tprevious        = np.array(data['previous'])\n",
        "\n",
        "data['age'] = np.log(data['age'])\n",
        "Tage        = np.array(data['age'])\n",
        "\n",
        "data['day'] = np.log(data['day'])\n",
        "Tday        = np.array(data['day'])\n",
        "\n",
        "data['duration'] = np.log(data['duration'])\n",
        "Tduration        = np.array(data['duration'])\n",
        "\n",
        "data['campaign'] = np.log(data['campaign'])\n",
        "Tcampaign        = np.array(data['campaign'])\n",
        "\n",
        "# Ahora revisamos c√≥mo se distribuyeron despu√©s de haber sido transfromadas\n",
        "fig, axs2 = plt.subplots(2,4, figsize=(10,7))\n",
        "\n",
        "axs2[0,0].hist(Tage, bins=40)\n",
        "axs2[0,0].set_xlabel('Age Transformed')\n",
        "\n",
        "axs2[0,1].hist(Tbalance, bins=20)\n",
        "axs2[0,1].set_xlabel('Balance Transformed')\n",
        "\n",
        "axs2[0,2].hist(Tday, bins=40)\n",
        "axs2[0,2].set_xlabel('Day Transformed')\n",
        "\n",
        "axs2[0,3].hist(Tduration, bins=40)\n",
        "axs2[0,3].set_xlabel('Duration Transformed')\n",
        "\n",
        "axs2[1,0].hist(Tcampaign, bins=20)\n",
        "axs2[1,0].set_xlabel('Campaign Transformed')\n",
        "\n",
        "axs2[1,1].hist(Tdays, bins=40)\n",
        "axs2[1,1].set_xlabel('pDays Transformed')\n",
        "\n",
        "axs2[1,2].hist(Tprevious, bins=40)\n",
        "axs2[1,2].set_xlabel('Previous Transformed')\n",
        "\n",
        "# 8.\tConsidera la variable ‚Äúy‚Äù como la variable de salida y el resto de las variables como las variables de entrada.\n",
        "# 9.\tParticiona los datos en los conjuntos de entrenamiento, validaci√≥n y prueba en 60%, 20% y 20%, respectivamente.\n",
        "X = data[['age','job','balance','day','month','duration','campaign','pdays','previous','marital_divorced','marital_married','marital_single','education_primary','education_secondary','education_tertiary','education_unknown','default_no','default_yes','housing_no','housing_yes','loan_no','loan_yes','poutcome_failure','poutcome_other','poutcome_success','poutcome_unknown','contact_cellular','contact_telephone','contact_unknown']]\n",
        "Y = data[['y']]\n",
        "\n",
        "X_train, X_val_test, Y_train, Y_val_test = train_test_split(X, Y, train_size=.60)\n",
        "X_val, X_test, Y_val, Y_test = train_test_split(X_val_test, Y_val_test, train_size=.50)\n",
        "\n",
        "# 10a.\tAplica el modelo Regresi√≥n Log√≠stica en el conjunto de entrenamiento.\n",
        "clf = LogisticRegression(C= 1.0, solver ='newton-cg', max_iter=1000)\n",
        "MRL = clf.fit(X_train, Y_train)\n",
        "\n",
        "# 10b. Valida el modelo con las predicciones del conjunto de validaci√≥n y su matriz de confusi√≥n.\n",
        "print(\"Exactitud Modelo de Regresi√≥n Log√≠stica = \", MRL.score(X_val, Y_val))\n",
        "print(\"\")\n",
        "\n",
        "print(\"Iniciando pruebas con diferentes valores de par√°metro C\")\n",
        "print(\"-----------------------------------------------------------------------\")\n",
        "from sklearn.metrics import ConfusionMatrixDisplay\n",
        "\n",
        "Conf_Mtrx_01 = ConfusionMatrixDisplay.from_estimator(MRL, X_test, Y_test)\n",
        "\n",
        "# 10c. Ajusta los par√°metros del modelo hasta obtener tu mejor resultado.\n",
        "# Se usar√°n diferentes valores de C (desde 0.25 hasta 2 en incrementos de 0.25)\n",
        "Ci = [i for i in range(25, 225, 25)]\n",
        "train_scores, val_scores = list(), list()\n",
        "train_errors, val_errors = list(), list()\n",
        "\n",
        "for i in Ci:\n",
        "  mod_clf = LogisticRegression(C= (i/100.0), solver ='newton-cg', max_iter=1000)\n",
        "\n",
        "  mod_clf.fit(X_train, np.ravel(Y_train))\n",
        "\n",
        "  # Predicciones y m√©tricas con el conjunto de entrenamiento:\n",
        "  train_yhat = mod_clf.predict(X_train)\n",
        "  train_loss =  sum(abs(np.ravel(Y_train) - train_yhat)) / Y_train.shape[0]\n",
        "  train_errors.append(train_loss)\n",
        "  train_acc = 1 - train_loss\n",
        "  train_scores.append(train_acc)\n",
        "\n",
        "  # Predicciones y m√©tricas con el conjunto de validaci√≥n:\n",
        "  val_yhat = mod_clf.predict(X_val)\n",
        "  val_loss = sum(abs(np.ravel(Y_val) - val_yhat)) / Y_val.shape[0]\n",
        "  val_errors.append(val_loss)\n",
        "  val_acc = 1 - val_loss\n",
        "  val_scores.append(val_acc)\n",
        "\n",
        "  # Mostramos el resultado de las m√©tricas en cada iteraci√≥n\n",
        "  print('C = %.2f => trainacc: %.3f, testacc: %.3f, trainloss: %.3f, testloss: %.3f'\n",
        "        % (i/100, train_acc, val_acc, train_loss, val_loss))\n",
        "\n",
        "# Generamos la gr√°fica de la exactitud para predecir, usando los datos de entrenamiento y de validaci√≥n\n",
        "fig, axs = plt.subplots()\n",
        "plt.plot(Ci, train_scores, '-o', label='Train')\n",
        "plt.plot(Ci, val_scores, '-o', label='Val')\n",
        "plt.title(\"C vs Exactitud\")\n",
        "plt.xlabel(\"C\")\n",
        "plt.ylabel(\"Exactitud\")\n",
        "plt.legend()\n",
        "\n",
        "fig, axse = plt.subplots()\n",
        "plt.plot(Ci, train_errors, '-o', label='Train')\n",
        "plt.plot(Ci, val_errors, '-o', label='Val')\n",
        "plt.title(\"C vs Errores\")\n",
        "plt.xlabel(\"C\")\n",
        "plt.ylabel(\"Errores\")\n",
        "plt.legend()\n",
        "\n",
        "# Para esta corrida, el mejor pron√≥stico fue con un C=0.5\n",
        "# Se ajusta el par√°metro C a 0.5\n",
        "clf2 = LogisticRegression(C= 0.5, solver ='newton-cg', max_iter=1000)\n",
        "MRL2 = clf2.fit(X_train, Y_train)\n",
        "\n",
        "print(\"Exactitud del Modelo de Regresi√≥n Log√≠stica despu√©s de ajustar par√°metros = \", MRL2.score(X_val, Y_val))\n",
        "print(\"-----------------------------------------------------------------------\")\n",
        "\n",
        "print(\"Matriz de confusi√≥n de Regresi√≥n Log√≠stica despu√©s de ajustar par√°metros\")\n",
        "Conf_Mtrx_02 = ConfusionMatrixDisplay.from_estimator(MRL2, X_test, Y_test)\n",
        "\n",
        "# 11a.\tAplica el modelo Red Neuronal en el conjunto de entrenamiento.\n",
        "print(\"Iniciando an√°lisis usando Redes Neuronales...\")\n",
        "print(\"-----------------------------------------------------------------------\")\n",
        "print(\"Primer modelo con 1 capa de 5 neuronas\")\n",
        "\n",
        "model = MLPClassifier(hidden_layer_sizes = (5),\n",
        "                      max_iter=1000,\n",
        "                      learning_rate_init=0.001)\n",
        "\n",
        "model.fit(X_train, np.ravel(Y_train))\n",
        "\n",
        "train_yhat = model.predict(X_train)\n",
        "train_loss =  sum(abs(np.ravel(Y_train) - train_yhat)) / Y_train.shape[0]\n",
        "train_errors.append(train_loss)\n",
        "train_acc = 1 - train_loss\n",
        "train_scores.append(train_acc)\n",
        "\n",
        "# Predicciones y m√©tricas con el conjunto de validaci√≥n:\n",
        "val_yhat = model.predict(X_val)\n",
        "val_loss = sum(abs(np.ravel(Y_val) - val_yhat)) / Y_val.shape[0]\n",
        "val_errors.append(val_loss)\n",
        "val_acc = 1 - val_loss\n",
        "val_scores.append(val_acc)\n",
        "\n",
        "# 11b. Valida el modelo con las predicciones del conjunto de validaci√≥n y su matriz de confusi√≥n.\n",
        "print('Exactitud del modelo Red Neuronal con 1 capa de 5 neuronas:', model.score(X_test, Y_test))\n",
        "\n",
        "print(\"Matriz de confusi√≥n Red Neuronal de 1 capa con 5 neuronas\")\n",
        "Conf_Mtrx_RN01 = ConfusionMatrixDisplay.from_estimator(model, X_test, Y_test)\n",
        "\n",
        "print( \"-----------------------------------------------------------------------\")\n",
        "print(\"Analizando modelos con 1 o 2 capas y diferente cantidad de neuronas por capa\")\n",
        "print(\"------------------------------------------------------------------------\")\n",
        "\n",
        "# 11c. Ajusta los par√°metros del modelo hasta obtener tu mejor modelo, entre ellos el n√∫mero de neuronas y capas ocultas.\n",
        "train_scores, val_scores = list(), list()\n",
        "train_errors, val_errors = list(), list()\n",
        "\n",
        "# Inicializamos variables para armado de combinaciones de capas y neuronas por capa\n",
        "wl = list()\n",
        "lst = list()\n",
        "i = 1\n",
        "# Inicio de armado de combinaciones\n",
        "# El resultado ser√° 1 capa con 5 neuronas, otra capa con 10 neuronas y 2 capas con 5 y 10 neuronas\n",
        "for c in range(1, 3, 1):\n",
        "    for n in range(5, 11, 5):\n",
        "        if (i < c ):\n",
        "            wl.append(n)\n",
        "            i = i + 1\n",
        "        else:\n",
        "            wl.append(n)\n",
        "            lst.append(wl)\n",
        "            wl = []\n",
        "            i = 1\n",
        "\n",
        "for i in lst:\n",
        "  model = MLPClassifier(hidden_layer_sizes = (i),\n",
        "                        max_iter=1000,\n",
        "                        learning_rate_init=0.001)\n",
        "\n",
        "  model.fit(X_train, np.ravel(Y_train))\n",
        "\n",
        "  # Predicciones y m√©tricas con el conjunto de entrenamiento:\n",
        "  train_yhat = model.predict(X_train)\n",
        "  train_loss =  sum(abs(np.ravel(Y_train) - train_yhat)) / Y_train.shape[0]\n",
        "  train_errors.append(train_loss)\n",
        "  train_acc = 1 - train_loss\n",
        "  train_scores.append(train_acc)\n",
        "\n",
        "  # Predicciones y m√©tricas con el conjunto de validaci√≥n:\n",
        "  val_yhat = model.predict(X_val)\n",
        "  val_loss = sum(abs(np.ravel(Y_val) - val_yhat)) / Y_val.shape[0]\n",
        "  val_errors.append(val_loss)\n",
        "  val_acc = 1 - val_loss\n",
        "  val_scores.append(val_acc)\n",
        "\n",
        "  # Mostramos el resultado de las m√©tricas en cada iteraci√≥n\n",
        "  print(i, '> trainacc: %.3f, testacc: %.3f, trainloss: %.3f, testloss: %.3f'\n",
        "        % (train_acc, val_acc, train_loss, val_loss))\n",
        "\n",
        "fig, axs = plt.subplots()\n",
        "plt.plot([5,10,15], train_scores, '-o', label='Train')\n",
        "plt.plot([5,10,15], val_scores, '-o', label='Val')\n",
        "\n",
        "plt.legend()\n",
        "\n",
        "plt.title('Neuronas vs Exactitud')\n",
        "plt.xlabel('Neuronas en modelo MLP')\n",
        "plt.ylabel('Exactitud')\n",
        "plt.show()\n",
        "\n",
        "plt.plot([50,100,150], train_errors, '-o', label='Train')\n",
        "plt.plot([50,100,150], val_errors, '-o', label='Val')\n",
        "plt.legend()\n",
        "\n",
        "plt.title('Neuronas vs Error')\n",
        "plt.xlabel('Neuronas en modelo MLP')\n",
        "plt.ylabel('Error')\n",
        "plt.show()\n",
        "\n",
        "# El mejor valor obtenido fue con 1 capa con 10 neuronas\n",
        "model = MLPClassifier(hidden_layer_sizes = (10),\n",
        "                      max_iter=1000,\n",
        "                      learning_rate_init=0.001)\n",
        "\n",
        "model.fit(X_train, np.ravel(Y_train))\n",
        "\n",
        "train_yhat = model.predict(X_train)\n",
        "train_loss =  sum(abs(np.ravel(Y_train) - train_yhat)) / Y_train.shape[0]\n",
        "train_errors.append(train_loss)\n",
        "train_acc = 1 - train_loss\n",
        "train_scores.append(train_acc)\n",
        "\n",
        "# Predicciones y m√©tricas con el conjunto de validaci√≥n:\n",
        "val_yhat = model.predict(X_val)\n",
        "val_loss = sum(abs(np.ravel(Y_val) - val_yhat)) / Y_val.shape[0]\n",
        "val_errors.append(val_loss)\n",
        "val_acc = 1 - val_loss\n",
        "val_scores.append(val_acc)\n",
        "\n",
        "# 12.\tSelecciona el mejor modelo encontrado en los incisos anteriores\n",
        "# y utiliza el conjunto de prueba para obtener el desempe√±o final del modelo y su matriz de confusi√≥n.\n",
        "print('Exactitud del modelo Red Neuronal con 2 capas con 5 y 10 neuronas:', model.score(X_test, Y_test))\n",
        "print()\n",
        "print(\"Matriz de confusi√≥n Red Neuronal de 2 capas con 5 y 10 neuronas\")\n",
        "Conf_Mtrx_RN02 = ConfusionMatrixDisplay.from_estimator(model, X_test, Y_test)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMzZx5KDcxNW5CNd3XDMYwJ",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}